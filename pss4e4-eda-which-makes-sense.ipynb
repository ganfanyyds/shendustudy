{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":72489,"databundleVersionId":8096274,"sourceType":"competition"},{"sourceId":74776,"databundleVersionId":8172961,"sourceType":"competition"}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\nfrom colorama import Fore, Style\nimport xgboost\nimport lightgbm\nimport catboost\nimport os\nimport datetime\nimport pickle\n\nfrom sklearn.base import clone\nfrom sklearn.model_selection import KFold, StratifiedKFold \nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.compose import TransformedTargetRegressor, ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, LabelEncoder\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.linear_model import Ridge, LinearRegression, Lasso\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, RandomForestClassifier, HistGradientBoostingRegressor, BaggingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_log_error","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-28T13:34:51.366514Z","iopub.execute_input":"2024-04-28T13:34:51.366926Z","iopub.status.idle":"2024-04-28T13:34:56.998225Z","shell.execute_reply.started":"2024-04-28T13:34:51.366894Z","shell.execute_reply":"2024-04-28T13:34:56.996567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration\n# Produce a submission file (you can set this to false if you only\n# want to see the cross-validation results)\nCOMPUTE_TEST_PRED = True\n\n# Participate in the official competition or the surrogate competition\nOFFICIAL_COMPETITION = True\n\n# Add the original data source to the synthetic data for training\n# Recommendation for the official competition: set USE_ORIGINAL_DATA = True\n# Recommendation for the surrogate competition: set USE_ORIGINAL_DATA = False\nUSE_ORIGINAL_DATA = OFFICIAL_COMPETITION\n\n# Containers for results\noof, test_pred = {}, {}","metadata":{"execution":{"iopub.status.busy":"2024-04-28T13:34:57.000568Z","iopub.execute_input":"2024-04-28T13:34:57.001252Z","iopub.status.idle":"2024-04-28T13:34:57.008208Z","shell.execute_reply.started":"2024-04-28T13:34:57.001181Z","shell.execute_reply":"2024-04-28T13:34:57.006497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if OFFICIAL_COMPETITION:\n    train = pd.read_csv('/kaggle/input/playground-series-s4e4/train.csv', index_col='id')\n    test = pd.read_csv('/kaggle/input/playground-series-s4e4/test.csv', index_col='id')\nelse:\n    train = pd.read_csv('/kaggle/input/surrogate-playground-series-s4e4/train.csv', index_col='id')\n    test = pd.read_csv('/kaggle/input/surrogate-playground-series-s4e4/test.csv', index_col='id')\n    \ntrain['Sex'] = train.Sex.astype('category')\ntest['Sex'] = test.Sex.astype(train.Sex.dtype)\n\nif not train.isna().any().any():\n    print('There are no missing values in train.')\nif not test.isna().any().any():\n    print('There are no missing values in test.')\n    \nprint(f\"Train shape: {train.shape}   test shape: {test.shape}\")\n    \nnumeric_features = ['Length', 'Diameter', 'Height', 'Whole weight', 'Whole weight.1', 'Whole weight.2', 'Shell weight']\nnumeric_vars = numeric_features + ['Rings']\n\ntrain","metadata":{"execution":{"iopub.status.busy":"2024-04-28T13:34:57.009728Z","iopub.execute_input":"2024-04-28T13:34:57.010167Z","iopub.status.idle":"2024-04-28T13:34:57.399294Z","shell.execute_reply.started":"2024-04-28T13:34:57.01013Z","shell.execute_reply":"2024-04-28T13:34:57.397782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can add the original dataset to the synthetic data for training. Although with only 4177 samples it is twenty times smaller than the competition dataset, its addition improves the scores of all models.\n\nThanks to @iqbalsyahakbar [for the code](https://www.kaggle.com/competitions/playground-series-s4e4/discussion/488082).","metadata":{}},{"cell_type":"code","source":"if USE_ORIGINAL_DATA:\n    !pip install ucimlrepo\n\n    from ucimlrepo import fetch_ucirepo \n\n    # fetch dataset \n    abalone = fetch_ucirepo(id=1) \n\n    # data (as pandas dataframes)\n    original_dataset = pd.concat([abalone.data.features, abalone.data.targets], axis = 1).rename({\n        'Whole_weight' : 'Whole weight',\n        'Shucked_weight' : 'Whole weight.1',\n        'Viscera_weight' : 'Whole weight.2',\n        'Shell_weight' : 'Shell weight'\n    }, axis = 1)\n    original_dataset['Sex'] = original_dataset.Sex.astype(train.Sex.dtype)\n    print(\"Original dataset shape:\", original_dataset.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T13:34:57.401885Z","iopub.execute_input":"2024-04-28T13:34:57.402227Z","iopub.status.idle":"2024-04-28T13:35:16.302785Z","shell.execute_reply.started":"2024-04-28T13:34:57.402199Z","shell.execute_reply":"2024-04-28T13:35:16.301409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cc = np.corrcoef(train[numeric_vars], rowvar=False)\nsns.heatmap(cc, center=0, cmap='coolwarm', annot=True,\n            xticklabels=numeric_vars, yticklabels=numeric_vars)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T13:35:16.304499Z","iopub.execute_input":"2024-04-28T13:35:16.304963Z","iopub.status.idle":"2024-04-28T13:35:16.880108Z","shell.execute_reply.started":"2024-04-28T13:35:16.304913Z","shell.execute_reply":"2024-04-28T13:35:16.878739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target distribution","metadata":{}},{"cell_type":"code","source":"vc = train.Rings.value_counts()\nplt.figure(figsize=(6, 2))\nplt.bar(vc.index, vc)\nplt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T13:35:16.881727Z","iopub.execute_input":"2024-04-28T13:35:16.882858Z","iopub.status.idle":"2024-04-28T13:35:17.166151Z","shell.execute_reply.started":"2024-04-28T13:35:16.882803Z","shell.execute_reply":"2024-04-28T13:35:17.164907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insight:**\n- Because all training targets are between 1 and 29, we may clip all predictions to the interval \\[1, 29\\]. Good models, however, won't need this clipping.","metadata":{}},{"cell_type":"code","source":"log_features = []\nfor col in numeric_features:\n    train[f'log_{col}'] = np.log1p(train[col])\n    test[f'log_{col}'] = np.log1p(test[col])\n    if USE_ORIGINAL_DATA:\n        original_dataset[f'log_{col}'] = np.log1p(original_dataset[col])\n    log_features.append(f'log_{col}')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T13:35:17.167767Z","iopub.execute_input":"2024-04-28T13:35:17.168197Z","iopub.status.idle":"2024-04-28T13:35:17.209227Z","shell.execute_reply.started":"2024-04-28T13:35:17.168158Z","shell.execute_reply":"2024-04-28T13:35:17.207914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-validation\n\nTo ensure that our cross-validation results are consistent, we'll use the same function for cross-validating all models.\n\nNotice that in cross-validation, we first split the dataset and then add the original data only to the training dataset. The validation dataset consists purely of competition data. This setup lets us correctly assess whether the original data are useful or harmful.","metadata":{}},{"cell_type":"code","source":"kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n\ndef cross_validate(model, label, features=test.columns, n_repeats=1):\n    \"\"\"Compute out-of-fold and test predictions for a given model.\n    \n    Out-of-fold and test predictions are stored in the global variables\n    oof and test_pred, respectively.\n    \n    If n_repeats > 1, the model is trained several times with different seeds.\n    \n    All predictions are clipped to the interval [1, 29].\n    \"\"\"\n    start_time = datetime.datetime.now()\n    scores = []\n    oof_preds = np.full_like(train.Rings, np.nan, dtype=float)\n    for fold, (idx_tr, idx_va) in enumerate(kf.split(train, train.Rings)):\n        X_tr = train.iloc[idx_tr][features]\n        X_va = train.iloc[idx_va][features]\n        y_tr = train.iloc[idx_tr].Rings\n        y_va = train.iloc[idx_va].Rings\n        \n        if USE_ORIGINAL_DATA:\n            X_tr = pd.concat([X_tr, original_dataset[features]], axis=0)\n            y_tr = pd.concat([y_tr, original_dataset.Rings], axis=0)\n            \n        y_pred = np.zeros_like(y_va, dtype=float)\n        for i in range(n_repeats):\n            m = clone(model)\n            if n_repeats > 1:\n                mm = m\n                if isinstance(mm, Pipeline):\n                    mm = mm[-1]\n                if isinstance(mm, TransformedTargetRegressor):\n                    mm = mm.regressor\n                mm.set_params(random_state=i)\n            m.fit(X_tr, y_tr)\n            y_pred += m.predict(X_va)\n        y_pred /= n_repeats\n        y_pred = y_pred.clip(1, 29)\n        \n#         residuals = np.log1p(y_va) - np.log1p(y_pred)\n#         plt.figure(figsize=(6, 2))\n#         plt.scatter(y_pred, residuals, s=1)\n#         plt.axhline(0, color='k')\n#         plt.show()\n        \n        score = mean_squared_log_error(y_va, y_pred, squared=False)\n        print(f\"# Fold {fold}: RMSLE={score:.5f}\")\n        scores.append(score)\n        oof_preds[idx_va] = y_pred\n    elapsed_time = datetime.datetime.now() - start_time\n    print(f\"{Fore.GREEN}# Overall: {np.array(scores).mean():.5f} {label}\"\n          f\"   {int(np.round(elapsed_time.total_seconds() / 60))} min{Style.RESET_ALL}\")\n    oof[label] = oof_preds\n    \n    if COMPUTE_TEST_PRED:\n        # Retrain n_repeats times with the whole dataset and average\n        y_pred = np.zeros(len(test), dtype=float)\n        X_tr = train[features]\n        y_tr = train.Rings\n        if USE_ORIGINAL_DATA:\n            X_tr = pd.concat([X_tr, original_dataset[features]], axis=0)\n            y_tr = pd.concat([y_tr, original_dataset.Rings], axis=0)\n        for i in range(n_repeats):\n            m = clone(model)\n            if n_repeats > 1:\n                mm = m\n                if isinstance(mm, Pipeline):\n                    mm = mm[-1]\n                if isinstance(mm, TransformedTargetRegressor):\n                    mm = mm.regressor\n                mm.set_params(random_state=i)\n            m.fit(X_tr, y_tr)\n            y_pred += m.predict(test[features])\n        y_pred /= n_repeats\n        y_pred = y_pred.clip(1, 29)\n        test_pred[label] = y_pred\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T13:35:17.211975Z","iopub.execute_input":"2024-04-28T13:35:17.212538Z","iopub.status.idle":"2024-04-28T13:35:17.232287Z","shell.execute_reply.started":"2024-04-28T13:35:17.212502Z","shell.execute_reply":"2024-04-28T13:35:17.230925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PolynomialFeatures + Ridge\nmodel = make_pipeline(ColumnTransformer([('ohe', OneHotEncoder(drop='first'), ['Sex'])],\n                                        remainder='passthrough'),\n                      StandardScaler(),\n                      PolynomialFeatures(degree=3),\n                      TransformedTargetRegressor(Ridge(100),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1))\ncross_validate(model, 'Poly-Ridge', numeric_features + log_features + ['Sex'])\n# Overall: 0.15275 Poly-Ridge   0 min","metadata":{"execution":{"iopub.status.busy":"2024-04-28T10:27:17.567155Z","iopub.execute_input":"2024-04-28T10:27:17.567582Z","iopub.status.idle":"2024-04-28T10:27:33.147979Z","shell.execute_reply.started":"2024-04-28T10:27:17.567543Z","shell.execute_reply":"2024-04-28T10:27:33.143502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Nystroem transformer + Ridge\nmodel = make_pipeline(ColumnTransformer([('ohe', OneHotEncoder(drop='first'), ['Sex'])],\n                                        remainder='passthrough'),\n                      StandardScaler(),\n                      Nystroem(n_components=500),\n                      TransformedTargetRegressor(Ridge(0.1),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1))\ncross_validate(model, 'Nystroem-Ridge', numeric_features + log_features + ['Sex'])\n# Overall: 0.15162 Nystroem-Ridge   0 min","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:04:52.954905Z","iopub.execute_input":"2024-04-28T11:04:52.955293Z","iopub.status.idle":"2024-04-28T11:05:08.027491Z","shell.execute_reply.started":"2024-04-28T11:04:52.955265Z","shell.execute_reply":"2024-04-28T11:05:08.025935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`KNeighborsRegressor` seems to be the worst of all models I looked at for this competition: ","metadata":{}},{"cell_type":"code","source":"# K nearest neighbors\nmodel = make_pipeline(ColumnTransformer([('ohe', OneHotEncoder(drop='first'), ['Sex'])],\n                                        remainder='passthrough'),\n                      StandardScaler(),\n                      TransformedTargetRegressor(KNeighborsRegressor(n_neighbors=50),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1))\ncross_validate(model, 'KNN', log_features + ['Sex'])\n# Overall: 0.15448 KNN   0 min","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:05:08.035442Z","iopub.execute_input":"2024-04-28T11:05:08.040391Z","iopub.status.idle":"2024-04-28T11:05:22.333457Z","shell.execute_reply.started":"2024-04-28T11:05:08.040303Z","shell.execute_reply":"2024-04-28T11:05:22.332241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A random forest is always worth a try:","metadata":{}},{"cell_type":"code","source":"# Random forest regressor\nmodel = make_pipeline(ColumnTransformer([('ohe', OneHotEncoder(drop='first'), ['Sex'])],\n                                        remainder='passthrough'),\n                      TransformedTargetRegressor(RandomForestRegressor(n_estimators=200, min_samples_leaf=8, max_features=5),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1))\ncross_validate(model, 'Random forest', log_features + ['Sex'])\n# Overall: 0.14943 Random forest   3 min","metadata":{"execution":{"iopub.status.busy":"2024-04-26T04:48:38.51292Z","iopub.execute_input":"2024-04-26T04:48:38.513249Z","iopub.status.idle":"2024-04-26T04:51:25.19988Z","shell.execute_reply.started":"2024-04-26T04:48:38.513223Z","shell.execute_reply":"2024-04-26T04:51:25.198869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As explained by @siukeitin in [Making RMSLE-optimized predictions from a classifier model](https://www.kaggle.com/competitions/playground-series-s4e4/discussion/491339), we can solve our task with a multi-class classifier instead of a regressor because the target has only 28 unique values:","metadata":{}},{"cell_type":"code","source":"# Random forest classifier\ndef RMSLEOptimized(base_classifier_class):\n    \"\"\"\n    Return a regressor class with a predict() method which\n    is based on base_classifier_class.\n    \n    From https://www.kaggle.com/competitions/playground-series-s4e4/discussion/491339\"\"\"\n    class RMSLEOptimizedClassifier(base_classifier_class):    \n        def fit(self, X, y):\n            enc = LabelEncoder().fit(y)\n            self.c_ = np.log1p(enc.classes_)\n            super().fit(X, enc.transform(y))\n            return self\n        \n        def predict(self, X):\n            return np.expm1(np.sum(super().predict_proba(X)*self.c_, axis=1))\n\n    return RMSLEOptimizedClassifier\n\nmodel = make_pipeline(ColumnTransformer([('ohe', OneHotEncoder(drop='first'), ['Sex'])],\n                                        remainder='passthrough'),\n                      RMSLEOptimized(RandomForestClassifier)(n_estimators=200, min_samples_leaf=4, max_features=5))\ncross_validate(model, 'Random forest classifier', log_features + ['Sex'])\n# Overall: 0.14984 Random forest classifier   5 min","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-04-26T04:51:25.200884Z","iopub.execute_input":"2024-04-26T04:51:25.201197Z","iopub.status.idle":"2024-04-26T04:56:03.855168Z","shell.execute_reply.started":"2024-04-26T04:51:25.201173Z","shell.execute_reply":"2024-04-26T04:56:03.853998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ExtraTreesRegressor\nmodel = make_pipeline(ColumnTransformer([('ohe', OneHotEncoder(drop='first'), ['Sex'])],\n                                        remainder='passthrough'),\n                      TransformedTargetRegressor(ExtraTreesRegressor(n_estimators=200, min_samples_leaf=7),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1))\ncross_validate(model, 'ExtraTrees', log_features + ['Sex'])\n# Overall: 0.15025 ExtraTrees   2 min","metadata":{"execution":{"iopub.status.busy":"2024-04-26T04:56:03.856692Z","iopub.execute_input":"2024-04-26T04:56:03.857113Z","iopub.status.idle":"2024-04-26T04:57:40.652693Z","shell.execute_reply.started":"2024-04-26T04:56:03.857076Z","shell.execute_reply":"2024-04-26T04:57:40.651567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HistGradientBoostingRegressor\n# Hyperparameters were tuned with Optuna\nhgb_params = {'max_iter': 300, 'max_leaf_nodes': 43, 'early_stopping': False, 'learning_rate': 0.08019987638525192, 'min_samples_leaf': 37} # 0.14916\nmodel = make_pipeline(ColumnTransformer([('ohe', OneHotEncoder(), ['Sex'])],\n                                        remainder='passthrough'),\n                      TransformedTargetRegressor(HistGradientBoostingRegressor(**hgb_params),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1))\ncross_validate(model, 'HGB', numeric_features + ['Sex'])\n# Overall: 0.14912 HGB   0 min","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:04:10.222372Z","iopub.execute_input":"2024-04-28T11:04:10.222924Z","iopub.status.idle":"2024-04-28T11:04:34.239569Z","shell.execute_reply.started":"2024-04-28T11:04:10.22288Z","shell.execute_reply":"2024-04-28T11:04:34.238135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We implement three XGBoost models:\n- The first XGBoost model optimizes MSE and has a log-transformed target.\n- The second XGBoost model optimizes MSLE directly and doesn't need a target transformation as suggested in @siukeitin's discussion [log1p-transformed target + MSE objective vs MSLE objective](https://www.kaggle.com/competitions/playground-series-s4e4/discussion/488283).\n- The third XGBoost model is a classifier for 28 classes making [RMSLE-optimized predictions from a classifier model](https://www.kaggle.com/competitions/playground-series-s4e4/discussion/491339).\n\nThe classifier is much slower than the two regression models because it builds 28 trees per iteration (i.e., one tree per class and iteration) rather than only one per iteration.","metadata":{}},{"cell_type":"code","source":"# XGBoost with transformed target and RMSE objective\n# Hyperparameters were tuned with Optuna\nxgb_params = {'grow_policy': 'lossguide', 'n_estimators': 300, 'learning_rate': 0.09471805900675286, 'max_bin': 2048, 'max_depth': 8, 'reg_lambda': 33.33929116223339, 'min_child_weight': 27.048028004026204, 'colsample_bytree': 0.6105442825961575, 'objective': 'reg:squarederror', 'tree_method': 'hist', 'gamma': 0, 'enable_categorical': True} # 0.14859\nmodel = TransformedTargetRegressor(xgboost.XGBRegressor(**xgb_params),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1)\ncross_validate(model, 'XGBoost', numeric_features + ['Sex'], n_repeats=5)\n# Overall: 0.14837 XGBoost   1 min with max_bin=256 (default)\n# Overall: 0.14774 XGBoost   1 min with max_bin=1024\n# Overall: 0.14754 XGBoost   1 min with max_bin=2048","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:46:11.374949Z","iopub.execute_input":"2024-04-28T14:46:11.375439Z","iopub.status.idle":"2024-04-28T14:47:36.335179Z","shell.execute_reply.started":"2024-04-28T14:46:11.375405Z","shell.execute_reply":"2024-04-28T14:47:36.333791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBoost with original target and RMSLE objective\n# Hyperparameters were tuned with Optuna\nxgb_params = {'grow_policy': 'depthwise', 'n_estimators': 500, 'learning_rate': 0.0896765799823656, 'max_bin': 2048, 'max_depth': 8, 'reg_lambda': 1.003764844090402, 'min_child_weight': 0.20627702562667777, 'colsample_bytree': 0.5142803343048419, 'objective': 'reg:squaredlogerror', 'tree_method': 'hist', 'gamma': 0, 'enable_categorical': True} # 0.14875\nmodel = xgboost.XGBRegressor(**xgb_params)\ncross_validate(model, 'XGBoost-RMSLE', numeric_features + ['Sex'], n_repeats=5)\n# Overall: 0.14866 XGBoost-RMSLE   2 min with max_bin=256 (default)\n# Overall: 0.14800 XGBoost-RMSLE   2 min with max_bin=2048","metadata":{"execution":{"iopub.status.busy":"2024-04-28T13:44:06.222035Z","iopub.execute_input":"2024-04-28T13:44:06.222542Z","iopub.status.idle":"2024-04-28T13:45:50.369002Z","shell.execute_reply.started":"2024-04-28T13:44:06.222508Z","shell.execute_reply":"2024-04-28T13:45:50.367971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBoost classifier\nxgb_params = {'grow_policy': 'depthwise', 'n_estimators': 300, 'learning_rate': 0.1119313681903098, 'max_depth': 10, 'reg_lambda': 7.736050413670695, 'min_child_weight': 5.36254997449309, 'colsample_bytree': 0.6287870737865981, 'objective': 'multi:softmax', 'tree_method': 'hist', 'gamma': 0, 'enable_categorical': True} # 0.14897\nmodel = RMSLEOptimized(xgboost.XGBClassifier)(**xgb_params)\ncross_validate(model, 'XGBoost-classifier', numeric_features + ['Sex'], n_repeats=5)\n# Overall: 0.14877 XGBoost classifier   25 min","metadata":{"execution":{"iopub.status.busy":"2024-04-26T05:00:10.122979Z","iopub.execute_input":"2024-04-26T05:00:10.12385Z","iopub.status.idle":"2024-04-26T05:18:38.619862Z","shell.execute_reply.started":"2024-04-26T05:00:10.123822Z","shell.execute_reply":"2024-04-26T05:18:38.618744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LightGBM\n# Hyperparameters were tuned with Optuna\nlgbm_params = {'n_estimators': 1000, 'learning_rate': 0.038622511348472645, 'max_bin': 2048, 'colsample_bytree': 0.5757189042456357, 'reg_lambda': 0.09664116733307193, 'min_child_samples': 87, 'num_leaves': 43, 'verbose': -1} # 0.14804\nmodel = TransformedTargetRegressor(lightgbm.LGBMRegressor(**lgbm_params),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1)\ncross_validate(model, 'LightGBM', numeric_features + ['Sex'], n_repeats=5)\n# Overall: 0.14782 LightGBM   4 min with max_bin=255 (default)\n# Overall: 0.14748 LightGBM   4 min with max_bin=2048","metadata":{"execution":{"iopub.status.busy":"2024-04-28T13:47:18.335775Z","iopub.execute_input":"2024-04-28T13:47:18.336188Z","iopub.status.idle":"2024-04-28T13:51:34.371145Z","shell.execute_reply.started":"2024-04-28T13:47:18.33616Z","shell.execute_reply":"2024-04-28T13:51:34.369827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There [was a discussion](https://www.kaggle.com/competitions/playground-series-s4e4/discussion/491703) about whether a sqrt transformation of the target was better than the log transformation. The following LightGBM model uses the sqrt transformation and has a higher error than the model above. Log transformation is the way to go!","metadata":{}},{"cell_type":"code","source":"# LightGBM with sqrt transformation\n# Hyperparameters were tuned with Optuna\nlgbm_params = {'n_estimators': 1000, 'learning_rate': 0.038622511348472645, 'max_bin': 2048, 'colsample_bytree': 0.5757189042456357, 'reg_lambda': 0.09664116733307193, 'min_child_samples': 87, 'num_leaves': 43, 'verbose': -1} # 0.14804\nmodel = TransformedTargetRegressor(lightgbm.LGBMRegressor(**lgbm_params),\n                                                 func=np.sqrt,\n                                                 inverse_func=np.square)\ncross_validate(model, 'LightGBM-sqrt', numeric_features + ['Sex'], n_repeats=5)\n# Overall: 0.14796 LightGBM-sqrt   4 min with max_bin=255 (default)\n# Overall: 0.14761 LightGBM-sqrt   4 min with max_bin=1024\n# Overall: 0.14758 LightGBM-sqrt   4 min with max_bin=2048","metadata":{"execution":{"iopub.status.busy":"2024-04-28T13:58:42.523575Z","iopub.execute_input":"2024-04-28T13:58:42.524012Z","iopub.status.idle":"2024-04-28T14:02:47.492085Z","shell.execute_reply.started":"2024-04-28T13:58:42.523981Z","shell.execute_reply":"2024-04-28T14:02:47.490916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll now compare Catboost for two different grow policies.","metadata":{}},{"cell_type":"code","source":"# Catboost\n# Hyperparameters were tuned with Optuna for grow_policy='Lossguide'\ncb_params = {'grow_policy': 'Lossguide', 'n_estimators': 1000, 'learning_rate': 0.12444528932682379, 'max_bin': 2048, 'l2_leaf_reg': 41.57232155127747, 'min_child_samples': 75, 'colsample_bylevel': 0.9931075066636142, 'subsample': 0.9885992818939339, 'random_strength': 0.09223106939759793, 'boost_from_average': True, 'loss_function': 'RMSE', 'bootstrap_type': 'Bernoulli', 'cat_features': ['Sex'], 'verbose': False} # 0.14815\nmodel = TransformedTargetRegressor(catboost.CatBoostRegressor(**cb_params),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1)\ncross_validate(model, 'Catboost-LG', log_features + ['Sex'], n_repeats=5)\n# Overall: 0.14800 Catboost-LG   14 min with max_bin=254 (default)\n# Overall: 0.14762 Catboost-LG   17 min with max_bin=2048","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:14:33.591493Z","iopub.execute_input":"2024-04-28T14:14:33.591974Z","iopub.status.idle":"2024-04-28T14:31:11.327182Z","shell.execute_reply.started":"2024-04-28T14:14:33.59194Z","shell.execute_reply":"2024-04-28T14:31:11.326088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Catboost\n# Hyperparameters were tuned with Optuna for grow_policy='SymmetricTree'\ncb_params = {'grow_policy': 'SymmetricTree', 'n_estimators': 1000, 'learning_rate': 0.128912681527133, 'max_bin': 2048, 'l2_leaf_reg': 1.836927907521674, 'max_depth': 6, 'colsample_bylevel': 0.6775373040510968, 'random_strength': 0, 'boost_from_average': True, 'loss_function': 'RMSE', 'cat_features': ['Sex'], 'verbose': False} # 0.14847\nmodel = TransformedTargetRegressor(catboost.CatBoostRegressor(**cb_params),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1)\ncross_validate(model, 'Catboost-ST', log_features + ['Sex'], n_repeats=5)\n# Overall: 0.14826 Catboost-ST   7 min with max_bin=254 (default)\n# Overall: 0.14800 Catboost-ST   9 min with max_bin=2048\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:00:41.036746Z","iopub.execute_input":"2024-04-28T15:00:41.037244Z","iopub.status.idle":"2024-04-28T15:09:13.637813Z","shell.execute_reply.started":"2024-04-28T15:00:41.037207Z","shell.execute_reply":"2024-04-28T15:09:13.636632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`MLPRegressor` is scikit-learn's implementation of a simple neural network. For our dataset, it seems to be too simple: it gives a similar score like ridge regression.\n\nThe network has six hidden layers with sizes as defined in the parameter `hidden_layer_sizes`.","metadata":{}},{"cell_type":"code","source":"# Multi-layer perceptron\nmlp_params = {'learning_rate_init': 1e-4, 'max_iter': 200, 'hidden_layer_sizes': (128, 64, 32, 32, 16, 16), 'early_stopping': True, 'tol': 1e-5}\nmodel = make_pipeline(ColumnTransformer([('ohe', OneHotEncoder(), ['Sex'])],\n                                        remainder='passthrough'),\n                      StandardScaler(),\n                      TransformedTargetRegressor(MLPRegressor(**mlp_params),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1))\ncross_validate(model, f'MLP {mlp_params[\"hidden_layer_sizes\"]}', log_features + ['Sex'], n_repeats=5)\n# Overall: 0.15146 MLP (128, 64, 32, 32, 16, 16)   24 min","metadata":{"execution":{"iopub.status.busy":"2024-04-26T05:33:06.587583Z","iopub.execute_input":"2024-04-26T05:33:06.587888Z","iopub.status.idle":"2024-04-26T05:57:22.409302Z","shell.execute_reply.started":"2024-04-26T05:33:06.587861Z","shell.execute_reply":"2024-04-26T05:57:22.408205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"members = [name for name in oof.keys() if 'Stack' not in name]\n\nX = np.log1p(np.column_stack([oof[name] for name in members]))\nmodel = TransformedTargetRegressor(Ridge(positive=True, tol=1e-6),\n                                   func=np.log1p,\n                                   inverse_func=np.expm1)\nmodel.fit(X, train.Rings)\nprint('Ensemble weights')\nweights = pd.Series(model.regressor_.coef_, index=members)\nprint(weights)\nprint('Total weight:', weights.sum())\nprint('Intercept:', model.regressor_.intercept_)\noof['Stack'] = model.predict(X) # not really out-of-fold...\nprint(f\"Score: {mean_squared_log_error(train.Rings, oof['Stack'], squared=False):.5f}\")\n\n# Pie chart\nweights = weights[weights >= 0.005] # hide small weights in pie chart\nplt.pie(weights, labels=weights.index, autopct=\"%.0f%%\")\nplt.title('Ensemble weights')\nplt.show()\n\n# Test predictions\nif COMPUTE_TEST_PRED:\n    X = np.log1p(np.column_stack([test_pred[name] for name in members]))\n    test_pred['Stack'] = model.predict(X)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:12:20.203665Z","iopub.execute_input":"2024-04-28T15:12:20.204406Z","iopub.status.idle":"2024-04-28T15:12:20.460779Z","shell.execute_reply.started":"2024-04-28T15:12:20.204374Z","shell.execute_reply":"2024-04-28T15:12:20.459932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now save the oof values and the test predictions so that we can experiment with ensembling methods in another notebook:","metadata":{}},{"cell_type":"code","source":"with open('abalone_oof.pickle', 'wb') as f:\n    pickle.dump(oof, f)\nwith open('abalone_test_pred.pickle', 'wb') as f:\n    pickle.dump(test_pred, f)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T14:31:33.229302Z","iopub.execute_input":"2024-04-28T14:31:33.229743Z","iopub.status.idle":"2024-04-28T14:31:33.243921Z","shell.execute_reply.started":"2024-04-28T14:31:33.22971Z","shell.execute_reply":"2024-04-28T14:31:33.242556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation\n\nThe bar chart shows that for the given dataset, LightGBM, Catboost and XGBoost give the best predictions. The ensemble ('Stack') outperforms every single model.","metadata":{}},{"cell_type":"code","source":"result_list = []\nfor label in oof.keys():\n    score = mean_squared_log_error(train.Rings, oof[label], squared=False)\n    result_list.append((label, score))\nresult_df = pd.DataFrame(result_list, columns=['label', 'score'])\nresult_df.sort_values('score', inplace=True)\n\nplt.figure(figsize=(12, len(result_df) * 0.4 + 0.4))\nbars = plt.barh(np.arange(len(result_df)), result_df.score, color='lightgreen')\nplt.gca().bar_label(bars, fmt='%.5f')\nplt.yticks(np.arange(len(result_df)), result_df.label)\nplt.gca().invert_yaxis()\nif OFFICIAL_COMPETITION:\n    plt.xlim(0.14, 0.16)\nelse:\n    plt.xlim(0.16, 0.17)\nplt.xlabel('5-fold cv root mean squared log error (lower is better)')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-28T15:12:36.563145Z","iopub.execute_input":"2024-04-28T15:12:36.563555Z","iopub.status.idle":"2024-04-28T15:12:36.925122Z","shell.execute_reply.started":"2024-04-28T15:12:36.563527Z","shell.execute_reply":"2024-04-28T15:12:36.923709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"if COMPUTE_TEST_PRED:\n    sub = pd.Series(test_pred['Stack'], index=test.index, name='Rings')\n    filename = 'submission.csv' if OFFICIAL_COMPETITION else 'submission_surrogate.csv'\n    sub.to_csv(filename)\n    os.system(f\"head {filename}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-26T05:57:23.21846Z","iopub.execute_input":"2024-04-26T05:57:23.219072Z","iopub.status.idle":"2024-04-26T05:57:23.22305Z","shell.execute_reply.started":"2024-04-26T05:57:23.21903Z","shell.execute_reply":"2024-04-26T05:57:23.222412Z"},"trusted":true},"execution_count":null,"outputs":[]}]}